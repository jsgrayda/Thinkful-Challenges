{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1       1.0\n",
       "2       2.0\n",
       "3       1.0\n",
       "4       1.0\n",
       "6       1.0\n",
       "7       1.0\n",
       "8       2.0\n",
       "9       1.0\n",
       "10      1.0\n",
       "11      1.0\n",
       "12      1.0\n",
       "13      2.0\n",
       "14      1.0\n",
       "15      1.0\n",
       "16      1.0\n",
       "17      1.0\n",
       "18      2.0\n",
       "19      1.0\n",
       "20      1.0\n",
       "21      2.0\n",
       "22      1.0\n",
       "23      2.0\n",
       "24      1.0\n",
       "25      1.0\n",
       "26      1.0\n",
       "27      1.0\n",
       "28      1.0\n",
       "29      1.0\n",
       "30      2.0\n",
       "       ... \n",
       "8564    2.0\n",
       "8565    2.0\n",
       "8566    2.0\n",
       "8567    2.0\n",
       "8568    2.0\n",
       "8569    2.0\n",
       "8570    2.0\n",
       "8571    2.0\n",
       "8572    2.0\n",
       "8573    2.0\n",
       "8574    2.0\n",
       "8575    2.0\n",
       "8576    2.0\n",
       "8577    2.0\n",
       "8578    2.0\n",
       "8579    2.0\n",
       "8580    2.0\n",
       "8581    2.0\n",
       "8582    2.0\n",
       "8583    2.0\n",
       "8584    2.0\n",
       "8585    2.0\n",
       "8586    2.0\n",
       "8587    2.0\n",
       "8588    2.0\n",
       "8589    2.0\n",
       "8590    2.0\n",
       "8591    2.0\n",
       "8592    2.0\n",
       "8593    2.0\n",
       "Name: partner, Length: 8147, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['partner'] # I guess 1.0 or 2.0 indicates some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['partner'] - 1 # the minus 1 makes it so that the values in the row are 0.0 or 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for reference, might be useful\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.044189852700491\n",
      "Percent Type II errors: 0.1692580469176214\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.07116564417177915\n",
      "Percent Type II errors: 0.18036809815950922\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing iterations decreases error rates!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
